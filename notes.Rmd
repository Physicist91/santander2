---
title: "Personal Notes"
output: html_document
---

#Setting Up

```{r}
library(xgboost)
library(caret)
library(dplyr)
library(Matrix)
```


# Exploratory analysis

Read data and combine train and test into one dataframe.

```{r}
dat_train <- read.csv("train.csv", stringsAsFactors = FALSE)
dat_test <- read.csv("test.csv", stringsAsFactors = FALSE)

dat_test$TARGET <- -1
all_dat <- rbind(dat_train, dat_test)
```

Detect missing values.

```{r}
boxplot(all_dat$var3)
boxplot(all_dat[,grep('delta', names(all_dat))])
boxplot(all_dat$var36)
```

Multi-class variables

```{r}
barplot(table(all_dat$var36))
```

Binary variables
```{r}
for(i in grep('^ind_', names(all_dat)) ){
    print(table(all_dat[, i]))
}
```

age is the most important
```{r}
library(ggplot2)
qplot(var15, data=dat_train, colour=as.factor(TARGET), geom='density')
qplot(var15, data=dat_train[dat_train$var3 == 6, ], geom='density', col=as.factor(TARGET))
hist(all_dat$var15[all_dat$var3 == 2])
table(dat_train$TARGET)
table(dat_train$TARGET[dat_train$var15 >30])
```

saldo_var_30 is also very important.
ind seems to be an indicator of something about the customer!
Unhappy customer has low balance!

```{r}
qplot(saldo_var30, data=all_dat, fill=as.factor(ind_var30), geom='histogram')
qplot(num_var30_0, data=all_dat, fill=as.factor(ind_var30_0), geom='histogram')
qplot(saldo_var30, data=dat_train, color=as.factor(TARGET), geom='density')
table(dat_train$TARGET)
table(dat_train$TARGET[dat_train$saldo_var30 < 10000])
table(dat_train$TARGET[dat_train$saldo_var1 < 10])
table(dat_train$TARGET[dat_train$saldo_var5 < 10])
table(dat_train$TARGET[dat_train$saldo_var8 < 10])
densityplot(dat_train$saldo_var30[dat_train$TARGET == 1], col='red')
densityplot(dat_train$saldo_var1[dat_train$TARGET == 1], col='red')
densityplot(dat_train$saldo_var5[dat_train$TARGET == 1], col='red')
densityplot(dat_train$saldo_var8[dat_train$TARGET == 1], col='red')
```


# Data cleaning


## Removing constant variables

First is to encode missing values with NA.

```{r}
all_dat[all_dat$var3 == -999999, "var3"] <- NA
delta_vars <- names(all_dat)[grep('^delta', names(all_dat))]
for(i in delta_vars){
  all_dat[all_dat[, i] == 9999999999, i] <- NA
}
all_dat[all_dat$var36 == 99, "var36"] <- NA
all_dat[all_dat$num_var12_0 == 111, "num_var12_0"] <- NA
```

Next is to remove variables with zero variance.

```{r}
zeroVar <- nearZeroVar(all_dat, saveMetrics = TRUE)
names(all_dat)[zeroVar[,"zeroVar"]]
all_dat <- all_dat[, !zeroVar[, "zeroVar"]]
```


## Removing identical variables

```{r}
temp <- names(all_dat)[duplicated(lapply(all_dat, summary))]
cat(temp, sep="\n")
all_dat <- all_dat[!duplicated(lapply(all_dat, summary))]
cat("Deleted ", length(temp), " duplicated variables.")
```

## Removing correlated variables

Note: this prevents overfitting

```{r}
cor_v <- abs(cor(all_dat))
diag(cor_v) <- 0
cor_v[upper.tri(cor_v)] <- 0
cor_v <- as.data.frame(which(cor_v > 0.85, arr.ind = T))
cat(names(all_dat)[unique(cor_v$row)], sep="\n")
all_dat <- all_dat[,-unique(cor_v$row)]
cat("Deleted ", length(unique(cor_v$row)), " correlated variables.")
```


# Feature engineering

```{r}
# convert to categorical
all_dat$var36 <- as.factor(all_dat$var36)

# count of ind
all_dat$ind_count <- apply(all_dat[, grep('^ind', names(all_dat))], 1, function(x)(sum(x == 0)))

# balance/saldo small
all_dat$saldo0 <- apply(all_dat[, grep('^saldo', names(all_dat))], 1, function(x)(sum(x <= 10)))
all_dat$spain30 <- (all_dat$var15 > 30 & all_dat$var3 == 2) * 1
```

# Modeling

xgboost preprocessing

```{r}

# one-hot encoding
dummies <- dummyVars(~var36, data=all_dat)
ohe <- as.data.frame(predict(dummies, newdata=all_dat))
all_dat <- cbind(all_dat[, ! names(all_dat) %in% c('var36')], ohe)

# standardize NA to -9999 (required for dmatrix)
all_dat[is.na(all_dat$var3), "var3"] <- -9999
delta_vars <- names(all_dat)[grep('^delta', names(all_dat))]
for(i in delta_vars){
  all_dat[is.na(all_dat[, i]), i] <- -9999
}
var36s <- names(all_dat)[grep('var36', names(all_dat))]
for(i in var36s){
  all_dat[is.na(all_dat[, i]), i] <- -9999
}
all_dat[is.na(all_dat$num_var12_0), "num_var12_0"] <- -9999
all_dat[is.na(all_dat$spain30), "spain30"] <- -9999

train <- all_dat[all_dat$ID %in% dat_train$ID, ]
test <- all_dat[all_dat$ID %in% dat_test$ID, ]

names(all_dat)

y.train <- train$TARGET
train$ID <- NULL
train <- sparse.model.matrix(TARGET ~ .-1, data=train)
dtrain <- xgb.DMatrix(data=train, label=y.train, missing=-9999)

ID.test <- test$ID
test$ID <- NULL
test <- sparse.model.matrix(TARGET ~. -1, data=test)
```

xgboost algorithm

```{r}
param <- list(objective = "binary:logistic",
              booster = "gbtree",
			        eval_metric = "auc",
              nthread=2,
			        eta=0.02,
			        max_depth=5,
			        colsample_bytree=0.85,
			        subsample=0.95)

clf <- xgb.train(       params              = param, 
                        data = dtrain,
                        nrounds             = 350, 
                        verbose             = 2,
                        maximize            = TRUE
       )
```

# Feature importance

Variable importance as obtained from xgboost.

```{r}

Dimnames <- names(select(train, -ID, -TARGET))
importance_matrix <- xgb.importance(Dimnames,model= xgbmodel)

importance_matrix
```


# Tuning

Stepsize (eta) = 0.02 seems to be the best.

max.depth = 5 seems to be the best.

Optimize by nelder mead / grid search.

# Other notes

AGE = var15

number of months = '^num_meses*'

var3 is country of residence!

num_meses is number of months!

Impute from Hmisc?

var17 financing product?